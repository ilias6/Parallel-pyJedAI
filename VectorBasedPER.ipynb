{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Nta3pWbLiM"
   },
   "source": [
    "## Imports & Data Module Definition\n",
    "\n",
    "Here you can define:\n",
    "\n",
    "\n",
    "*   Dataset Paths\n",
    "*   Ground Truth Path\n",
    "*   Attributes to keep from each dataset\n",
    "*   ID Column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y7X4hagJbCd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import networkx\n",
    "from networkx import draw, Graph\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "from pyjedai.utils import (\n",
    "    text_cleaning_method,\n",
    "    print_clusters,\n",
    "    print_blocks,\n",
    "    print_candidate_pairs\n",
    ")\n",
    "from pyjedai.evaluation import Evaluation, write\n",
    "from pyjedai.datamodel import Data\n",
    "\n",
    "D1_PATH = \"data/test/ccer/abt_100.csv\"\n",
    "D2_PATH = \"data/test/ccer/buy_100.csv\"\n",
    "GT_PATH = \"data/test/ccer/gt_100.csv\"\n",
    "\n",
    "D1_ATTRS = ['id','name','description'] \n",
    "D1_ID_COL = 'id'\n",
    "D2_ATTRS = ['id','name','description'] \n",
    "D2_ID_COL = 'id'\n",
    "\n",
    "d1 = pd.read_csv(D1_PATH, sep='|', engine='python', na_filter=False).astype(str)\n",
    "d2 = pd.read_csv(D2_PATH, sep='|', engine='python', na_filter=False).astype(str)\n",
    "gt = pd.read_csv(GT_PATH, sep='|', engine='python')\n",
    "\n",
    "data = Data(\n",
    "    dataset_1=d1,\n",
    "    attributes_1=D1_ATTRS,\n",
    "    id_column_name_1=D1_ID_COL,\n",
    "    dataset_2=d2,\n",
    "    attributes_2=D2_ATTRS,\n",
    "    id_column_name_2=D2_ID_COL,\n",
    "    ground_truth=gt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMyu8szFe2dw"
   },
   "source": [
    "## BLOCK CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xIfVWlupeqO2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f302266422ac4085994d6e0dfbb5bf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Q-Grams Blocking:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************************************************************\n",
      "                                         Μethod:  Q-Grams Blocking\n",
      "***************************************************************************************************************************\n",
      "Method name: Q-Grams Blocking\n",
      "Parameters: \n",
      "\tQ-Gramms: 6\n",
      "Runtime: 0.0338 seconds\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Performance:\n",
      "\tPrecision:      1.78% \n",
      "\tRecall:       100.00%\n",
      "\tF1-score:       3.51%\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Classification report:\n",
      "\tTrue positives: 49\n",
      "\tFalse positives: 2697\n",
      "\tTrue negatives: 7303\n",
      "\tFalse negatives: 0\n",
      "\tTotal comparisons: 2746\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Precision %': 1.7844136926438456,\n",
       " 'Recall %': 100.0,\n",
       " 'F1 %': 3.5062611806797856,\n",
       " 'True Positives': 49,\n",
       " 'False Positives': 2697,\n",
       " 'True Negatives': 7303,\n",
       " 'False Negatives': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyjedai.block_building import (\n",
    "    StandardBlocking,\n",
    "    QGramsBlocking,\n",
    "    ExtendedQGramsBlocking,\n",
    "    SuffixArraysBlocking,\n",
    "    ExtendedSuffixArraysBlocking,\n",
    ")\n",
    "\n",
    "qgb = QGramsBlocking(qgrams=6)\n",
    "blocks = qgb.build_blocks(data, attributes_1=['name'], attributes_2=['name'])\n",
    "qgb.evaluate(blocks, with_classification_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYluDqfCeP13"
   },
   "source": [
    "## RUN THE BELOW IF YOU WANT TO LOWER EXECUTION TIME\n",
    "Following methods may:\n",
    "* Lower the amount of blocks\n",
    "* Lower the cardinality of blocks\n",
    "\n",
    "This will result in smaller search space, but will lower the number of true positives that will finally be emitted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wmdSJmUPfBg2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bb384bdc0643b3aeea635f60b5a881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Block Filtering:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyjedai.block_cleaning import BlockFiltering\n",
    "\n",
    "bf = BlockFiltering(ratio=0.8)\n",
    "filtered_blocks = bf.process(blocks, data, tqdm_disable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yV1TceT7fF-Z"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f0dbb622814d70ada21fb16de88f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Block Purging:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************************************************************\n",
      "                                         Μethod:  Block Purging\n",
      "***************************************************************************************************************************\n",
      "Method name: Block Purging\n",
      "Parameters: \n",
      "\tSmoothing factor: 1.025\n",
      "\tMax Comparisons per Block: 22.0\n",
      "Runtime: 0.0087 seconds\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Performance:\n",
      "\tPrecision:      7.33% \n",
      "\tRecall:        85.71%\n",
      "\tF1-score:      13.50%\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Classification report:\n",
      "\tTrue positives: 42\n",
      "\tFalse positives: 531\n",
      "\tTrue negatives: 9462\n",
      "\tFalse negatives: 7\n",
      "\tTotal comparisons: 573\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Precision %': 7.329842931937172,\n",
       " 'Recall %': 85.71428571428571,\n",
       " 'F1 %': 13.504823151125404,\n",
       " 'True Positives': 42,\n",
       " 'False Positives': 531,\n",
       " 'True Negatives': 9462,\n",
       " 'False Negatives': 7}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyjedai.block_cleaning import BlockPurging\n",
    "\n",
    "cbbp = BlockPurging()\n",
    "cleaned_blocks = cbbp.process(filtered_blocks, data, tqdm_disable=False)\n",
    "cbbp.evaluate(cleaned_blocks, with_classification_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eA5u5bO5dlUK"
   },
   "source": [
    "## ADJUST THE BELOW GLOBAL VARIABLES ACCORDINGLY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Bo2Z1wpoVyVd"
   },
   "outputs": [],
   "source": [
    "# the total number of pairs that will be emitted\n",
    "# Final amount may be lower or higher (duplicate emissions have been purged / upper bound for neighborhood budget is taken)\n",
    "BUDGET = 5000\n",
    "# When this option is enabled, the context of budget changes\n",
    "# In most algorithms, budget is ignored\n",
    "# BUT IN THE CASE OF VECTOR BASED PER - If Below True -> Budget = Size of neighborhood per entity\n",
    "STOP_EMISSION_WHEN_ALL_TRUE_POSITIVES_FOUND = False\n",
    "vectorizers = ['word2vec', 'fasttext', 'doc2vec', 'glove', 'bert', 'distilbert', 'roberta', 'xlnet', 'albert', 'smpnet', 'st5', 'sent_glove', 'sdistilroberta', 'sminilm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6c40JaRY4zJ"
   },
   "source": [
    "## Hybrid Approach\n",
    "\n",
    "For each vectorizer, we produce the corresponding embeddings.\n",
    "For each entity, we produce candidate pairs using faiss.\n",
    "We emit the candidate pairs following the hybrid approach.\n",
    "Specifically, we sort the entities in descending order of their neighborhood average weight and we emit the top comparison for each entity.\n",
    "Subsequently, we iterate over all the remaining candidates for current pair in descending order of their similarity score. We move to the next entity, till all have been examined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QSk_rf1tWdxn"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EmbeddingsNNBPM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m hb_matchers \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vectorizer \u001b[38;5;129;01min\u001b[39;00m vectorizers:\n\u001b[0;32m----> 6\u001b[0m   hb_matchers[vectorizer] \u001b[38;5;241m=\u001b[39m \u001b[43mEmbeddingsNNBPM\u001b[49m(\n\u001b[1;32m      7\u001b[0m       budget \u001b[38;5;241m=\u001b[39m BUDGET,\n\u001b[1;32m      8\u001b[0m       vectorizer \u001b[38;5;241m=\u001b[39m vectorizer,\n\u001b[1;32m      9\u001b[0m       similarity_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfaiss\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m   )\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvectorizer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m   hb_matchers[vectorizer]\u001b[38;5;241m.\u001b[39mpredict(cleaned_blocks, data, tqdm_disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, method\u001b[38;5;241m=\u001b[39mMETHOD, emit_all_tps_stop\u001b[38;5;241m=\u001b[39mSTOP_EMISSION_WHEN_ALL_TRUE_POSITIVES_FOUND)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EmbeddingsNNBPM' is not defined"
     ]
    }
   ],
   "source": [
    "METHOD = 'HB'\n",
    "\n",
    "hb_matchers = {}\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "  hb_matchers[vectorizer] = EmbeddingsNNBPM(\n",
    "      budget = BUDGET,\n",
    "      vectorizer = vectorizer,\n",
    "      similarity_search = 'faiss'\n",
    "  )\n",
    "\n",
    "  print(f\"Vectorizer: {vectorizer}\")\n",
    "  hb_matchers[vectorizer].predict(cleaned_blocks, data, tqdm_disable=True, method=METHOD, emit_all_tps_stop=STOP_EMISSION_WHEN_ALL_TRUE_POSITIVES_FOUND)\n",
    "\n",
    "\n",
    "# Matchers and their vectorizer names are being stored (Vectorizer name will be used as matcher's name in the ROC graph)\n",
    "hb_matchers_data = []\n",
    "\n",
    "for vectorizer, matcher in hb_matchers.items():\n",
    "  hb_matchers_data.append((matcher, vectorizer))\n",
    "\n",
    "evaluator = Evaluation(data)\n",
    "# evaluator calculates cumulative recall for each emission, final normalized AUC and displays the ROC graph\n",
    "evaluator.evaluate_auc_roc(matchers_data = hb_matchers_data, proportional = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUDQUPu0aGWn"
   },
   "source": [
    "## DFS Approach\n",
    "\n",
    "For each vectorizer, we produce the corresponding embeddings.\n",
    "For each entity, we produce candidate pairs using faiss.\n",
    "We emit the candidate pairs following the DFS approach.\n",
    "Specifically, we sort the entities in descending order of their neighborhood average weight. We iterate over the sorted entities and for each one we emit all its candidates in descending order of their similarity score, before moving to the next entity's neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlRLOMzcZznV"
   },
   "outputs": [],
   "source": [
    "METHOD = 'DFS'\n",
    "\n",
    "dfs_matchers = {}\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "  dfs_matchers[vectorizer] = EmbeddingsNNBPM(\n",
    "      budget = BUDGET,\n",
    "      vectorizer = vectorizer,\n",
    "      similarity_search = 'faiss'\n",
    "  )\n",
    "\n",
    "  print(f\"Vectorizer: {vectorizer}\")\n",
    "  dfs_matchers[vectorizer].predict(cleaned_blocks, data, tqdm_disable=True, method=METHOD, emit_all_tps_stop=STOP_EMISSION_WHEN_ALL_TRUE_POSITIVES_FOUND)\n",
    "\n",
    "\n",
    "# Matchers and their vectorizer names are being stored (Vectorizer name will be used as matcher's name in the ROC graph)\n",
    "dfs_matchers_data = []\n",
    "\n",
    "for vectorizer, matcher in dfs_matchers.items():\n",
    "  dfs_matchers_data.append((matcher, vectorizer))\n",
    "\n",
    "evaluator = Evaluation(data)\n",
    "# evaluator calculates cumulative recall for each emission, final normalized AUC and displays the ROC graph\n",
    "evaluator.evaluate_auc_roc(matchers_data = dfs_matchers_data, proportional = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQIelc-mavNd"
   },
   "source": [
    "## BFS Approach\n",
    "\n",
    "For each vectorizer, we produce the corresponding embeddings.\n",
    "For each entity, we produce candidate pairs using faiss.\n",
    "We emit the candidate pairs following the DFS approach.\n",
    "Specifically, we sort the entities in descending order of their neighborhood average weight. We iterate over the sorted entities and for each one we emit its currently top candidate (it won't be considered again). We move to the next entity and we keep iterating over them, till there are no neighbors for any entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgRyeIMvZ0nO"
   },
   "outputs": [],
   "source": [
    "METHOD = 'BFS'\n",
    "\n",
    "bfs_matchers = {}\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "  bfs_matchers[vectorizer] = EmbeddingsNNBPM(\n",
    "      budget = BUDGET,\n",
    "      vectorizer = vectorizer,\n",
    "      similarity_search = 'faiss'\n",
    "  )\n",
    "\n",
    "  print(f\"Vectorizer: {vectorizer}\")\n",
    "  bfs_matchers[vectorizer].predict(cleaned_blocks, data, tqdm_disable=True, method=METHOD, emit_all_tps_stop=STOP_EMISSION_WHEN_ALL_TRUE_POSITIVES_FOUND)\n",
    "\n",
    "\n",
    "# Matchers and their vectorizer names are being stored (Vectorizer name will be used as matcher's name in the ROC graph)\n",
    "bfs_matchers_data = []\n",
    "\n",
    "for vectorizer, matcher in bfs_matchers.items():\n",
    "  bfs_matchers_data.append((matcher, vectorizer))\n",
    "\n",
    "evaluator = Evaluation(data)\n",
    "# evaluator calculates cumulative recall for each emission, final normalized AUC and displays the ROC graph\n",
    "evaluator.evaluate_auc_roc(matchers_data = bfs_matchers_data, proportional = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
